import os
import numpy as np
from sklearn.cluster import AgglomerativeClustering
from pyannote.audio import Model, Audio
from pyannote.audio.pipelines.speaker_verification import PretrainedSpeakerEmbedding
from pyannote.core import Segment
from pydub import AudioSegment
from transformers import pipeline
import uuid
import torch

# Constants
SEGMENTED_AUDIO_FOLDER = "segmented_audios"
TRANSCRIPTIONS_FOLDER = "transcriptions"
THRESHOLD = 30  # Maximum chunk length for transcription
NUM_SPEAKERS = 2  # Number of speakers in the audio

# Ensure necessary folders exist
os.makedirs(SEGMENTED_AUDIO_FOLDER, exist_ok=True)
os.makedirs(TRANSCRIPTIONS_FOLDER, exist_ok=True)

# Load Models
device = "cuda" if torch.cuda.is_available() else "cpu"
vad_model_fp = torch.hub._get_torch_home() + "/whisperx-vad-segmentation.bin"
vad_model = Model.from_pretrained(vad_model_fp)
embedding_model = PretrainedSpeakerEmbedding("speechbrain/spkrec-ecapa-voxceleb", device=torch.device(device))
audio = Audio()
model_name = "your-huggingface-model-name"
pipe = pipeline(task="automatic-speech-recognition", model=model_name, tokenizer=model_name, chunk_length_s=8, device=0)

# Function to extract embeddings for each segment
def segment_embedding(segment, path):
    start = segment.start
    end = segment.end
    waveform, sample_rate = audio.crop(path, Segment(start, end))
    return embedding_model(waveform[None])

# Function to merge VAD segments
def merge_segments(vad_segments, min_length=3.0, max_gap=1.0):
    timeline = vad_segments.get_timeline().support(collar=max_gap)
    merged_timeline = []
    for segment in timeline:
        if segment.duration >= min_length:
            merged_timeline.append(segment)
    return merged_timeline

# Function to transcribe audio chunks
def transcribe_with_chunking(segment_path, max_chunk_duration=THRESHOLD):
    audio_segment = AudioSegment.from_file(segment_path)
    duration = len(audio_segment) / 1000  # in seconds
    if duration <= max_chunk_duration:
        result = pipe(segment_path)
        return result["text"]
    else:
        chunks = []
        for start in range(0, int(duration), max_chunk_duration):
            end = min(start + max_chunk_duration, duration)
            chunk_audio = audio_segment[start * 1000:end * 1000]
            chunk_path = f"temp_chunk_{start}_{end}.wav"
            chunk_audio.export(chunk_path, format="wav")
            chunk_result = pipe(chunk_path)
            chunks.append(chunk_result["text"])
        return " ".join(chunks)

# Main function to process audio
def process_audio_with_diarization(file_path):
    print(f"Processing file: {file_path}")
    # Check file format and convert if needed
    if not file_path.lower().endswith(".wav"):
        converted_path = "audio.wav"
        os.system(f"ffmpeg -i {file_path} {converted_path} -y")
        file_path = converted_path

    # Apply VAD
    vad_pipeline = Model.from_pretrained(vad_model_fp)
    vad_segments = vad_pipeline.apply({"audio": file_path})
    merged_segments = merge_segments(vad_segments)

    # Extract embeddings and perform clustering
    embeddings = np.zeros((len(merged_segments), 192))
    for i, segment in enumerate(merged_segments):
        embeddings[i] = segment_embedding(segment, file_path)

    clustering = AgglomerativeClustering(n_clusters=NUM_SPEAKERS).fit(embeddings)
    labels = clustering.labels_

    # Process and save each segment
    transcription_results = []
    for i, segment in enumerate(merged_segments):
        start_ms = segment.start * 1000
        end_ms = segment.end * 1000
        segment_audio = AudioSegment.from_file(file_path)[start_ms:end_ms]
        segment_filename = f"segment_{uuid.uuid4().hex}.wav"
        segment_path = os.path.join(SEGMENTED_AUDIO_FOLDER, segment_filename)
        segment_audio.export(segment_path, format="wav")

        transcription = transcribe_with_chunking(segment_path, max_chunk_duration=THRESHOLD)
        transcription_results.append((labels[i], segment.start, segment.end, transcription))

    # Save the transcription file with speaker labels
    base_filename = os.path.splitext(os.path.basename(file_path))[0]
    transcription_path = os.path.join(TRANSCRIPTIONS_FOLDER, f"{base_filename}_transcript.txt")
    with open(transcription_path, "w", encoding="utf-8") as f:
        for speaker, start, end, text in transcription_results:
            f.write(f"\nSPEAKER {speaker + 1} {start:.2f}-{end:.2f} sec:\n")
            f.write(text + "\n")

    print(f"Transcription with speaker labels saved at: {transcription_path}")

# Test the function
if __name__ == "__main__":
    input_audio_path = "/path/to/your/audio/file.wav"  # Replace with your audio file path
    process_audio_with_diarization(input_audio_path)
